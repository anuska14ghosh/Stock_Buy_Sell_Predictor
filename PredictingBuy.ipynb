{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO90wBzVR8LNfusuyB5yuvp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuska14ghosh/Stock_Buy_Sell_Prredictor/blob/main/PredictingBuy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictive model making preductions on whether to buy or not to buy stocks\n",
        "###  **NIFTY-50 Stocks Dataset**\n",
        "**Comprehensive Dataset from India's Top Companies on the NSE**\n",
        "\n",
        "dataset is downloaded from the following link  [https://www.kaggle.com/datasets/iamsouravbanerjee/nifty50-stocks-dataset](https://)"
      ],
      "metadata": {
        "id": "zK2r-AwFzr_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib"
      ],
      "metadata": {
        "id": "2fa_HaWx3BnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/National_Stock_Exchange_of_India_Ltd.csv\")\n",
        "print(data.head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO7HBg0U3KxB",
        "outputId": "3deaac8a-a1ae-44de-cb5d-0d16f131db45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of         Symbol       Open       High        Low        LTP    Chng  % Chng  \\\n",
            "0   ADANIPORTS        750        766     713.25        715  -47.45   -6.22   \n",
            "1   ASIANPAINT   3,101.00   3,167.35   3,091.00   3,138.00   -6.25   -0.20   \n",
            "2     AXISBANK        669      674.9     660.45        661  -18.90   -2.78   \n",
            "3   BAJAJ-AUTO   3,370.00   3,383.50   3,320.00   3,335.00  -56.70   -1.67   \n",
            "4   BAJAJFINSV  17,200.00  17,237.20  16,610.00  16,684.00 -684.85   -3.94   \n",
            "5   BAJFINANCE   7,021.00   7,047.90   6,775.00   6,780.00 -345.80   -4.85   \n",
            "6   BHARTIARTL        763        763      733.1     735.85  -29.30   -3.83   \n",
            "7         BPCL     397.15      397.2        375      377.4  -22.70   -5.67   \n",
            "8    BRITANNIA   3,560.00   3,635.10   3,533.95   3,566.60   -6.80   -0.19   \n",
            "9        CIPLA        892     976.05     890.65        965   65.05    7.23   \n",
            "10   COALINDIA     157.75      159.4     155.35      155.9   -2.65   -1.67   \n",
            "11    DIVISLAB   4,770.00   5,077.70   4,756.75   4,940.00  140.20    2.92   \n",
            "12     DRREDDY   4,580.00   4,820.00   4,576.15   4,750.00  158.40    3.45   \n",
            "13   EICHERMOT   2,495.00   2,506.10   2,421.50   2,440.75  -79.65   -3.16   \n",
            "14      GRASIM   1,757.30   1,757.85   1,679.00   1,685.80  -80.95   -4.58   \n",
            "15     HCLTECH   1,120.00   1,126.00   1,103.30   1,111.65  -13.15   -1.17   \n",
            "16        HDFC   2,820.35   2,856.00   2,723.00   2,745.00 -122.75   -4.28   \n",
            "17    HDFCBANK   1,500.00   1,506.70   1,485.00   1,489.50  -36.45   -2.39   \n",
            "18    HDFCLIFE        685        689      667.1     669.75  -19.05   -2.77   \n",
            "19  HEROMOTOCO   2,580.00   2,589.70   2,505.15   2,526.80  -67.90   -2.62   \n",
            "20    HINDALCO      441.8      442.7      414.7      417.7  -29.35   -6.57   \n",
            "21  HINDUNILVR   2,344.00   2,365.00   2,325.20   2,340.90   -8.15   -0.35   \n",
            "22   ICICIBANK        739     742.05      718.6     720.45  -30.60   -4.07   \n",
            "23  INDUSINDBK        951     956.95        898     899.95  -59.35   -6.19   \n",
            "24        INFY   1,702.55   1,718.35   1,684.00   1,689.55  -32.85   -1.91   \n",
            "25         IOC      125.6      125.6      120.5     121.15   -4.50   -3.58   \n",
            "26         ITC      228.9     230.05      223.1      223.6   -7.70   -3.33   \n",
            "27    JSWSTEEL     668.25     672.55     624.25        630  -50.90   -7.48   \n",
            "28   KOTAKBANK   2,002.00   2,007.00   1,955.10   1,960.00  -75.10   -3.69   \n",
            "29          LT   1,820.00   1,841.75   1,768.60   1,781.00  -68.90   -3.72   \n",
            "30         M&M        885        885        843     855.05  -36.15   -4.06   \n",
            "31      MARUTI   7,520.00   7,520.00   7,130.00   7,150.00 -422.50   -5.58   \n",
            "32   NESTLEIND  19,148.85  19,434.10  18,982.50  19,250.00   71.95    0.38   \n",
            "33        NTPC      133.2     134.05        128     128.65   -6.55   -4.84   \n",
            "34        ONGC     152.25     152.25     146.25     147.75   -7.35   -4.74   \n",
            "35   POWERGRID     204.05     204.95      200.8      202.5   -1.75   -0.86   \n",
            "36    RELIANCE   2,467.80   2,477.60   2,401.50   2,405.10  -87.85   -3.52   \n",
            "37     SBILIFE   1,154.00   1,154.00   1,105.25   1,130.85  -28.65   -2.47   \n",
            "38        SBIN     486.25      487.9      467.1        470  -20.55   -4.19   \n",
            "39    SHREECEM  26,450.00  26,539.90  25,812.00  25,900.00 -770.50   -2.89   \n",
            "40   SUNPHARMA        775      798.9        762     767.25  -15.65   -2.00   \n",
            "41  TATACONSUM      800.2        805     763.15      769.9  -37.90   -4.69   \n",
            "42  TATAMOTORS        486     486.75        458      459.4  -33.35   -6.77   \n",
            "43   TATASTEEL   1,157.90   1,159.50   1,106.25   1,110.25  -63.40   -5.40   \n",
            "44         TCS   3,425.00   3,490.00   3,411.90   3,439.20   -6.70   -0.19   \n",
            "45       TECHM   1,544.00   1,550.00   1,510.15   1,519.00  -40.35   -2.59   \n",
            "46       TITAN   2,377.80   2,385.10   2,285.05   2,293.00 -104.80   -4.37   \n",
            "47  ULTRACEMCO   7,550.00   7,599.00   7,370.10   7,398.45 -210.35   -2.76   \n",
            "48         UPL        726        726        701      703.5  -23.80   -3.27   \n",
            "49       WIPRO        632      634.4     619.65      621.3  -15.40   -2.42   \n",
            "\n",
            "    Volume (lacs) Turnover (crs.)      52w H      52w L  365 d % chng  \\\n",
            "0           72.20          532.63        901      384.4         79.22   \n",
            "1           10.29          322.53   3,505.00   2,117.15         45.66   \n",
            "2          102.53             684      866.9      568.4         10.19   \n",
            "3            3.42          114.59   4,361.40   3,041.00          9.30   \n",
            "4            3.42          576.79  19,325.00   8,273.70         91.38   \n",
            "5           16.89        1,161.63   8,050.00   4,362.00         44.57   \n",
            "6          111.43          830.06      781.8     454.11         58.55   \n",
            "7          100.23          383.54        503        357         -1.22   \n",
            "8            3.73          133.23   4,153.00   3,317.30          0.30   \n",
            "9          144.59        1,380.90   1,005.00      726.5         31.89   \n",
            "10         118.30           185.5      203.8     123.25         25.78   \n",
            "11          15.71          775.37   5,425.10   3,153.30         42.39   \n",
            "12          10.72          508.97   5,614.60   4,135.00         -1.17   \n",
            "13           5.55          136.56   3,037.00   2,303.70         -5.95   \n",
            "14           7.48          127.84   1,893.00     840.05         99.95   \n",
            "15          22.07          246.06   1,377.75     814.35         34.79   \n",
            "16          33.53          927.88   3,021.10   2,179.30         25.27   \n",
            "17          93.12        1,394.10   1,725.00   1,342.00          6.18   \n",
            "18          22.37           151.4     775.65      617.4          0.70   \n",
            "19           6.85          174.04   3,629.05   2,505.15        -16.02   \n",
            "20         148.26          631.93     551.85     220.35         86.93   \n",
            "21          24.51          572.85   2,859.30   2,120.00          9.60   \n",
            "22         189.88        1,385.86        867      465.8         52.41   \n",
            "23          67.46          622.74   1,242.00        789          5.25   \n",
            "24          44.94          764.67   1,848.00   1,091.00         51.44   \n",
            "25          77.25           94.57      141.5         84         41.28   \n",
            "26         270.27          610.54      265.3      192.4         15.35   \n",
            "27          89.22          574.61      776.5        336         86.25   \n",
            "28          26.48          522.52   2,253.00   1,626.00          5.24   \n",
            "29          27.97          502.81   1,981.75   1,092.00         59.59   \n",
            "30          39.34          338.08        979     660.25         18.77   \n",
            "31          11.55          840.81   8,368.00   6,400.00          1.34   \n",
            "32           0.56          108.61  20,609.15  16,002.10          9.87   \n",
            "33         133.24          173.94      152.1      88.15         36.93   \n",
            "34         231.36          344.33     172.75      77.05         82.86   \n",
            "35          96.11          195.09     209.95     136.88          3.69   \n",
            "36          72.75        1,770.19   2,751.35   1,830.00         23.48   \n",
            "37          23.16          262.43   1,273.90      825.2         33.19   \n",
            "38         263.06        1,249.55      542.3     240.15         93.42   \n",
            "39           0.30           76.94  32,048.00  22,531.00          9.29   \n",
            "40          54.33          424.05        851      502.3         51.57   \n",
            "41          26.17          203.32        889     505.05         49.55   \n",
            "42         517.88        2,430.36      536.7      156.7        167.95   \n",
            "43         106.46        1,200.79   1,534.50      539.5        105.13   \n",
            "44          19.41          670.58   3,989.90   2,624.45         27.32   \n",
            "45          15.22          232.97   1,630.00      846.7         76.17   \n",
            "46          12.89          298.54   2,677.90   1,300.35         75.45   \n",
            "47           2.66          198.32   8,269.00   4,770.00         53.50   \n",
            "48          24.82          176.35      864.7     414.15         68.06   \n",
            "49          41.39          259.37     739.85     346.25         77.51   \n",
            "\n",
            "    30 d % chng  \n",
            "0         -4.65  \n",
            "1          5.66  \n",
            "2        -21.49  \n",
            "3        -12.05  \n",
            "4         -9.10  \n",
            "5        -13.69  \n",
            "6          5.70  \n",
            "7        -12.45  \n",
            "8         -3.42  \n",
            "9          6.34  \n",
            "10       -10.94  \n",
            "11        -1.57  \n",
            "12         1.80  \n",
            "13        -5.77  \n",
            "14        -3.08  \n",
            "15        -4.73  \n",
            "16        -5.72  \n",
            "17        -9.88  \n",
            "18        -2.94  \n",
            "19        -6.43  \n",
            "20       -14.06  \n",
            "21        -3.94  \n",
            "22       -13.14  \n",
            "23       -22.08  \n",
            "24        -0.83  \n",
            "25        -7.87  \n",
            "26        -5.53  \n",
            "27        -9.27  \n",
            "28       -11.35  \n",
            "29        -0.85  \n",
            "30        -4.42  \n",
            "31        -2.02  \n",
            "32         0.17  \n",
            "33       -10.16  \n",
            "34        -9.41  \n",
            "35         6.36  \n",
            "36        -9.62  \n",
            "37        -3.52  \n",
            "38        -8.30  \n",
            "39        -6.76  \n",
            "40        -5.69  \n",
            "41        -4.82  \n",
            "42        -9.68  \n",
            "43       -17.37  \n",
            "44        -1.25  \n",
            "45        -2.83  \n",
            "46        -6.59  \n",
            "47         1.78  \n",
            "48        -1.37  \n",
            "49        -7.01  >\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning Data**\n",
        "\n",
        "Cleaning the dataset by removing the commas and converting relevant columns to float data type"
      ],
      "metadata": {
        "id": "HF6wFCaX4JI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the data: removing commas and converting it to numeric types for better conversion\n",
        "data['Open'] = data['Open'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "data['High'] = data['High'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "data['Low'] = data['Low'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "data['LTP'] = data['LTP'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "data['Volume (lacs)'] = data['Volume (lacs)'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "data['Turnover (crs.)'] = data['Turnover (crs.)'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "data['Chng'] = data['Chng'].fillna('0').astype(str).str.replace(',', '').astype(float)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPXG8NMI3xwe",
        "outputId": "efdf3566-9338-4796-ad52-69889b9a41d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Symbol      Open      High       Low       LTP    Chng  % Chng  \\\n",
            "0   ADANIPORTS    750.00    766.00    713.25    715.00  -47.45   -6.22   \n",
            "1   ASIANPAINT   3101.00   3167.35   3091.00   3138.00   -6.25   -0.20   \n",
            "2     AXISBANK    669.00    674.90    660.45    661.00  -18.90   -2.78   \n",
            "3   BAJAJ-AUTO   3370.00   3383.50   3320.00   3335.00  -56.70   -1.67   \n",
            "4   BAJAJFINSV  17200.00  17237.20  16610.00  16684.00 -684.85   -3.94   \n",
            "5   BAJFINANCE   7021.00   7047.90   6775.00   6780.00 -345.80   -4.85   \n",
            "6   BHARTIARTL    763.00    763.00    733.10    735.85  -29.30   -3.83   \n",
            "7         BPCL    397.15    397.20    375.00    377.40  -22.70   -5.67   \n",
            "8    BRITANNIA   3560.00   3635.10   3533.95   3566.60   -6.80   -0.19   \n",
            "9        CIPLA    892.00    976.05    890.65    965.00   65.05    7.23   \n",
            "10   COALINDIA    157.75    159.40    155.35    155.90   -2.65   -1.67   \n",
            "11    DIVISLAB   4770.00   5077.70   4756.75   4940.00  140.20    2.92   \n",
            "12     DRREDDY   4580.00   4820.00   4576.15   4750.00  158.40    3.45   \n",
            "13   EICHERMOT   2495.00   2506.10   2421.50   2440.75  -79.65   -3.16   \n",
            "14      GRASIM   1757.30   1757.85   1679.00   1685.80  -80.95   -4.58   \n",
            "15     HCLTECH   1120.00   1126.00   1103.30   1111.65  -13.15   -1.17   \n",
            "16        HDFC   2820.35   2856.00   2723.00   2745.00 -122.75   -4.28   \n",
            "17    HDFCBANK   1500.00   1506.70   1485.00   1489.50  -36.45   -2.39   \n",
            "18    HDFCLIFE    685.00    689.00    667.10    669.75  -19.05   -2.77   \n",
            "19  HEROMOTOCO   2580.00   2589.70   2505.15   2526.80  -67.90   -2.62   \n",
            "20    HINDALCO    441.80    442.70    414.70    417.70  -29.35   -6.57   \n",
            "21  HINDUNILVR   2344.00   2365.00   2325.20   2340.90   -8.15   -0.35   \n",
            "22   ICICIBANK    739.00    742.05    718.60    720.45  -30.60   -4.07   \n",
            "23  INDUSINDBK    951.00    956.95    898.00    899.95  -59.35   -6.19   \n",
            "24        INFY   1702.55   1718.35   1684.00   1689.55  -32.85   -1.91   \n",
            "25         IOC    125.60    125.60    120.50    121.15   -4.50   -3.58   \n",
            "26         ITC    228.90    230.05    223.10    223.60   -7.70   -3.33   \n",
            "27    JSWSTEEL    668.25    672.55    624.25    630.00  -50.90   -7.48   \n",
            "28   KOTAKBANK   2002.00   2007.00   1955.10   1960.00  -75.10   -3.69   \n",
            "29          LT   1820.00   1841.75   1768.60   1781.00  -68.90   -3.72   \n",
            "30         M&M    885.00    885.00    843.00    855.05  -36.15   -4.06   \n",
            "31      MARUTI   7520.00   7520.00   7130.00   7150.00 -422.50   -5.58   \n",
            "32   NESTLEIND  19148.85  19434.10  18982.50  19250.00   71.95    0.38   \n",
            "33        NTPC    133.20    134.05    128.00    128.65   -6.55   -4.84   \n",
            "34        ONGC    152.25    152.25    146.25    147.75   -7.35   -4.74   \n",
            "35   POWERGRID    204.05    204.95    200.80    202.50   -1.75   -0.86   \n",
            "36    RELIANCE   2467.80   2477.60   2401.50   2405.10  -87.85   -3.52   \n",
            "37     SBILIFE   1154.00   1154.00   1105.25   1130.85  -28.65   -2.47   \n",
            "38        SBIN    486.25    487.90    467.10    470.00  -20.55   -4.19   \n",
            "39    SHREECEM  26450.00  26539.90  25812.00  25900.00 -770.50   -2.89   \n",
            "40   SUNPHARMA    775.00    798.90    762.00    767.25  -15.65   -2.00   \n",
            "41  TATACONSUM    800.20    805.00    763.15    769.90  -37.90   -4.69   \n",
            "42  TATAMOTORS    486.00    486.75    458.00    459.40  -33.35   -6.77   \n",
            "43   TATASTEEL   1157.90   1159.50   1106.25   1110.25  -63.40   -5.40   \n",
            "44         TCS   3425.00   3490.00   3411.90   3439.20   -6.70   -0.19   \n",
            "45       TECHM   1544.00   1550.00   1510.15   1519.00  -40.35   -2.59   \n",
            "46       TITAN   2377.80   2385.10   2285.05   2293.00 -104.80   -4.37   \n",
            "47  ULTRACEMCO   7550.00   7599.00   7370.10   7398.45 -210.35   -2.76   \n",
            "48         UPL    726.00    726.00    701.00    703.50  -23.80   -3.27   \n",
            "49       WIPRO    632.00    634.40    619.65    621.30  -15.40   -2.42   \n",
            "\n",
            "    Volume (lacs)  Turnover (crs.)      52w H      52w L  365 d % chng  \\\n",
            "0           72.20           532.63        901      384.4         79.22   \n",
            "1           10.29           322.53   3,505.00   2,117.15         45.66   \n",
            "2          102.53           684.00      866.9      568.4         10.19   \n",
            "3            3.42           114.59   4,361.40   3,041.00          9.30   \n",
            "4            3.42           576.79  19,325.00   8,273.70         91.38   \n",
            "5           16.89          1161.63   8,050.00   4,362.00         44.57   \n",
            "6          111.43           830.06      781.8     454.11         58.55   \n",
            "7          100.23           383.54        503        357         -1.22   \n",
            "8            3.73           133.23   4,153.00   3,317.30          0.30   \n",
            "9          144.59          1380.90   1,005.00      726.5         31.89   \n",
            "10         118.30           185.50      203.8     123.25         25.78   \n",
            "11          15.71           775.37   5,425.10   3,153.30         42.39   \n",
            "12          10.72           508.97   5,614.60   4,135.00         -1.17   \n",
            "13           5.55           136.56   3,037.00   2,303.70         -5.95   \n",
            "14           7.48           127.84   1,893.00     840.05         99.95   \n",
            "15          22.07           246.06   1,377.75     814.35         34.79   \n",
            "16          33.53           927.88   3,021.10   2,179.30         25.27   \n",
            "17          93.12          1394.10   1,725.00   1,342.00          6.18   \n",
            "18          22.37           151.40     775.65      617.4          0.70   \n",
            "19           6.85           174.04   3,629.05   2,505.15        -16.02   \n",
            "20         148.26           631.93     551.85     220.35         86.93   \n",
            "21          24.51           572.85   2,859.30   2,120.00          9.60   \n",
            "22         189.88          1385.86        867      465.8         52.41   \n",
            "23          67.46           622.74   1,242.00        789          5.25   \n",
            "24          44.94           764.67   1,848.00   1,091.00         51.44   \n",
            "25          77.25            94.57      141.5         84         41.28   \n",
            "26         270.27           610.54      265.3      192.4         15.35   \n",
            "27          89.22           574.61      776.5        336         86.25   \n",
            "28          26.48           522.52   2,253.00   1,626.00          5.24   \n",
            "29          27.97           502.81   1,981.75   1,092.00         59.59   \n",
            "30          39.34           338.08        979     660.25         18.77   \n",
            "31          11.55           840.81   8,368.00   6,400.00          1.34   \n",
            "32           0.56           108.61  20,609.15  16,002.10          9.87   \n",
            "33         133.24           173.94      152.1      88.15         36.93   \n",
            "34         231.36           344.33     172.75      77.05         82.86   \n",
            "35          96.11           195.09     209.95     136.88          3.69   \n",
            "36          72.75          1770.19   2,751.35   1,830.00         23.48   \n",
            "37          23.16           262.43   1,273.90      825.2         33.19   \n",
            "38         263.06          1249.55      542.3     240.15         93.42   \n",
            "39           0.30            76.94  32,048.00  22,531.00          9.29   \n",
            "40          54.33           424.05        851      502.3         51.57   \n",
            "41          26.17           203.32        889     505.05         49.55   \n",
            "42         517.88          2430.36      536.7      156.7        167.95   \n",
            "43         106.46          1200.79   1,534.50      539.5        105.13   \n",
            "44          19.41           670.58   3,989.90   2,624.45         27.32   \n",
            "45          15.22           232.97   1,630.00      846.7         76.17   \n",
            "46          12.89           298.54   2,677.90   1,300.35         75.45   \n",
            "47           2.66           198.32   8,269.00   4,770.00         53.50   \n",
            "48          24.82           176.35      864.7     414.15         68.06   \n",
            "49          41.39           259.37     739.85     346.25         77.51   \n",
            "\n",
            "    30 d % chng  \n",
            "0         -4.65  \n",
            "1          5.66  \n",
            "2        -21.49  \n",
            "3        -12.05  \n",
            "4         -9.10  \n",
            "5        -13.69  \n",
            "6          5.70  \n",
            "7        -12.45  \n",
            "8         -3.42  \n",
            "9          6.34  \n",
            "10       -10.94  \n",
            "11        -1.57  \n",
            "12         1.80  \n",
            "13        -5.77  \n",
            "14        -3.08  \n",
            "15        -4.73  \n",
            "16        -5.72  \n",
            "17        -9.88  \n",
            "18        -2.94  \n",
            "19        -6.43  \n",
            "20       -14.06  \n",
            "21        -3.94  \n",
            "22       -13.14  \n",
            "23       -22.08  \n",
            "24        -0.83  \n",
            "25        -7.87  \n",
            "26        -5.53  \n",
            "27        -9.27  \n",
            "28       -11.35  \n",
            "29        -0.85  \n",
            "30        -4.42  \n",
            "31        -2.02  \n",
            "32         0.17  \n",
            "33       -10.16  \n",
            "34        -9.41  \n",
            "35         6.36  \n",
            "36        -9.62  \n",
            "37        -3.52  \n",
            "38        -8.30  \n",
            "39        -6.76  \n",
            "40        -5.69  \n",
            "41        -4.82  \n",
            "42        -9.68  \n",
            "43       -17.37  \n",
            "44        -1.25  \n",
            "45        -2.83  \n",
            "46        -6.59  \n",
            "47         1.78  \n",
            "48        -1.37  \n",
            "49        -7.01  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection (Chng is the target variable we are trying to predict here )\n",
        "features = ['Open', 'High', 'Low', 'LTP', 'Volume (lacs)', 'Turnover (crs.)']\n",
        "X = data[features]"
      ],
      "metadata": {
        "id": "OARpxa_S4V8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** convert the \"Chng\" column into a binary target where 1 indicates a positive change (buy) and 0 indicates a non-positive change (do not buy).**"
      ],
      "metadata": {
        "id": "9jQd1FdO5q9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'Chng' to a binary classification target\n",
        "y = (data['Chng'] > 0).astype(int)\n",
        "# 1 if 'Chng' > 0, else 0"
      ],
      "metadata": {
        "id": "dbo1Sk5Z4me2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling**\n",
        "\n",
        "Scaling Features using StandardScaler to ensure they contribute equally to the optimization process."
      ],
      "metadata": {
        "id": "OwADzn0r-jO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "fdmeUAWT_MLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting data into 2 equal halves for training and testing**"
      ],
      "metadata": {
        "id": "4rlKAfCd40XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the data into training and testing sets (50/50)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "1nxgRGDK48ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a logistic regression model with increased iterations\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "#increasing iterations as max iteration would have exceed in the other case\n",
        "#The max_iter parameter is increased to 1000, allowing more iterations for the algorithm to converge."
      ],
      "metadata": {
        "id": "evJP6Zhf5D9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model on the training data\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "riJAoX6J9ZPF",
        "outputId": "dca2ea6f-9b88-4884-be8f-15ef0d16b05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained model and the scaler\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXpACrp-2Tki",
        "outputId": "0062d8cd-2ea6-4d44-dd3d-1868102e0163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the testing data\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "ph6Zm0RxATpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model's performance (accuracy)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXtz7ChpAgtw",
        "outputId": "230db31f-0506-4008-8793-0aa87c026f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the detailed classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Z8nBcmAmAZ",
        "outputId": "76662496-86ab-4a1a-a013-81dccee09ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93        22\n",
            "           1       0.50      0.67      0.57         3\n",
            "\n",
            "    accuracy                           0.88        25\n",
            "   macro avg       0.73      0.79      0.75        25\n",
            "weighted avg       0.90      0.88      0.89        25\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few predictions\n",
        "predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(predictions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhcTYEEiA0bL",
        "outputId": "91368787-f9c8-45e6-ee9a-fb7f6d139a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Actual  Predicted\n",
            "13       0          0\n",
            "39       0          1\n",
            "30       0          0\n",
            "45       0          0\n",
            "17       0          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make a decision based on prediction\n",
        "def buy_or_not(prediction):\n",
        "    return \"Buy\" if prediction == 1 else \"Do Not Buy\"\n"
      ],
      "metadata": {
        "id": "e7rJBae2A5pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with function to make predictions for new data\n",
        "def make_prediction(example_features):\n",
        "    # Load the trained model and the scaler\n",
        "    model = joblib.load('logistic_regression_model.pkl')\n",
        "    scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "    example_features_scaled = scaler.transform(example_features)\n",
        "    example_prediction = model.predict(example_features_scaled)\n",
        "    decision = buy_or_not(example_prediction[0])\n",
        "    return example_prediction[0], decision"
      ],
      "metadata": {
        "id": "FlnJAdsv2tL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "example_features = [[685, 689, 667.1, 669.75, 22.37, 151.4]] #taking HDFCLIFE\n",
        "example_features_scaled = scaler.transform(example_features)\n",
        "example_prediction = model.predict(example_features_scaled)\n",
        "decision = buy_or_not(example_prediction[0])\n",
        "print(f\"Prediction: {example_prediction[0]}, Decision: {decision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRixJNLvBYQv",
        "outputId": "d7e3a0b4-1210-463a-8a22-c4c65286c29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 0, Decision: Do Not Buy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Dask** it provides parallel computing capabilities for large datasets that do not fit into memory though very similar to pandas but designed to work with larger-than-memory datasets."
      ],
      "metadata": {
        "id": "vKu-X1l923Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Large dataset of Tata Motors**\n",
        "NIFTY-50 Stock Market Data (2000-2021)\n",
        "The dataset is downloaded from: [https://www.kaggle.com/datasets/rohanrao/nifty50-stock-market-data](https://)"
      ],
      "metadata": {
        "id": "6av0sr6C5RGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading new dataset using Dask\n",
        "import dask.dataframe as dd\n",
        "data = dd.read_csv(\"/content/TATAMOTORS.csv\")"
      ],
      "metadata": {
        "id": "wPXHzrDz21TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_features = [[207.4, 217.25, 207.4, 216.75, 676126, 1448780000]]\n",
        "prediction, decision = make_prediction(example_features)\n",
        "print(f\"Prediction: {prediction}, Decision: {decision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydjv5uhs3EjF",
        "outputId": "cf65084d-5956-4b95-dbfe-a16b1797099f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 0, Decision: Do Not Buy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}